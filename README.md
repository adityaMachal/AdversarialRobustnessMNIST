# AdversarialRobustnessMNIST
Investigating the robustness of various supervised learning models against adversarial attacks on the MNIST dataset. Implements the Fast Gradient Sign Method (FGSM) to generate adversarial examples and evaluates model performance under varying perturbation levels. Includes Jupyter Notebook with code, visualizations, and analysis.
